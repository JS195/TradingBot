{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the API\n",
    "\n",
    "API = \"api-fxpractice.oanda.com\"\n",
    "STREAM_API = \"stream-fxpractice.oanda.com\"\n",
    "ACCESS_TOKEN = \"06a31b21abc8fb4cbe9bbd8c5b7b1f4e-260aefea008b878b6bffbfebafc84b9f\"\n",
    "ACCOUNT_ID = \"101-004-26291813-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the data\n",
    "\n",
    "# Time format is year, month, day\n",
    "from_time = time.mktime(pd.to_datetime(\"2023-07-1-00-00-00\").timetuple())\n",
    "to_time = time.mktime(pd.to_datetime(\"2023-07-10-00-00-00\").timetuple())\n",
    "\n",
    "header = {\"Authorization\": \"Bearer \" + ACCESS_TOKEN}\n",
    "# Granularity is limited for the amount of data available, no more than 3000 entries give or take\n",
    "query = {\"from\": from_time, \"to\": to_time, \"granularity\": \"M5\"}\n",
    "\n",
    "INSTRUMENT = \"XAU_GBP\"\n",
    "CANDLES_PATH = f\"/v3/accounts/{ACCOUNT_ID}/instruments/{INSTRUMENT}/candles\"\n",
    "\n",
    "response = requests.get(\"https://\" + API + CANDLES_PATH, headers=header, params=query)\n",
    "\n",
    "def json_to_pd(json):\n",
    "    json_file = json.json()\n",
    "    price_json = json_file[\"candles\"]\n",
    "    times = []\n",
    "    close_price, high_price, low_price, open_price, volume = [], [], [], [], []\n",
    "\n",
    "    for candle in price_json:\n",
    "        times.append(candle[\"time\"])\n",
    "        close_price.append(float(candle[\"mid\"][\"c\"]))\n",
    "        high_price.append(float(candle[\"mid\"][\"h\"]))\n",
    "        low_price.append(float(candle[\"mid\"][\"l\"]))\n",
    "        open_price.append(float(candle[\"mid\"][\"o\"]))\n",
    "        volume.append(float(candle[\"volume\"]))\n",
    "\n",
    "    dataframe = pd.DataFrame({\"close\": close_price, \"high\": high_price, \"low\": low_price, \"open\": open_price, \"volume\": volume})\n",
    "    dataframe.index = pd.to_datetime(times)\n",
    "    return dataframe\n",
    "\n",
    "get_data = json_to_pd(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Preprocessing\n",
    "\n",
    "# Calculate the percentage change of the data\n",
    "def calculate_percentage_change(data):\n",
    "    data['open'] = data['open'].pct_change()\n",
    "    data['high'] = data['high'].pct_change()\n",
    "    data['low'] = data['low'].pct_change()\n",
    "    data['close'] = data['close'].pct_change()\n",
    "    data['volume'] = data['volume'].pct_change()\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "#percent_data = calculate_percentage_change(get_data)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "dataframe_scaled = pd.DataFrame(scaler.fit_transform(get_data), columns=get_data.columns, index=get_data.index)\n",
    "\n",
    "# Train, test and validation split\n",
    "train_end = int(0.8 * len(dataframe_scaled))\n",
    "val_end = int(0.9 * len(dataframe_scaled))\n",
    "\n",
    "df_train = dataframe_scaled.iloc[:train_end]\n",
    "df_val = dataframe_scaled.iloc[train_end: val_end]\n",
    "df_test = dataframe_scaled.iloc[val_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Technical Analysis, Statistical Analysis and Feature Engineering\n",
    "\n",
    "# Computing the RSI\n",
    "\n",
    "# Technical indicators\n",
    "\n",
    "# Lagged variables\n",
    "\n",
    "# Calendar features\n",
    "\n",
    "# Moving averages\n",
    "\n",
    "# Volatility measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Select a model used for time series analysis\n",
    "\n",
    "#Building the LTSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Validation and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Evaluatie the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Deployment and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Managing risk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introtoai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
